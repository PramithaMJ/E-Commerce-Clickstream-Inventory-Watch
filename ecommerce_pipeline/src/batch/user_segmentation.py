"""
User Segmentation Batch Job for E-Commerce Analytics.

This module implements a PySpark batch job that processes archived
clickstream data to segment users and generate analytics reports.

User Segmentation Logic:
    - Window Shoppers: Users with 0 purchases (views only)
    - Buyers: Users with ≥1 purchase

Analytics Outputs:
    - Top 5 Most Viewed Products
    - Conversion Rates per Product Category
    - User Segment Distribution

"""

import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Optional

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col,
    count,
    countDistinct,
    date_format,
    lit,
    round as spark_round,
    sum as spark_sum,
    when,
    desc,
    first,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class UserSegmentationJob:
    """PySpark batch job for user segmentation and analytics.

    This job reads Parquet files generated by the streaming layer
    and produces user segments and analytics reports.

    Attributes:
        spark: Active Spark session.
        input_path: Path to Parquet files.
        output_path: Path for report output.
        processing_date: Date to process (default: yesterday).

    Example:
        >>> job = UserSegmentationJob(
        ...     spark=SparkSession.builder.getOrCreate(),
        ...     input_path="/data/parquet",
        ...     output_path="/reports"
        ... )
        >>> results = job.run()
    """

    def __init__(
        self,
        spark: SparkSession,
        input_path: str,
        output_path: str,
        processing_date: Optional[datetime] = None
    ) -> None:
        """Initialize the user segmentation job.

        Args:
            spark: Active Spark session.
            input_path: Path to input Parquet files.
            output_path: Path for output reports.
            processing_date: Date to process (filters data).
        """
        self.spark = spark
        self.input_path = input_path
        self.output_path = output_path
        self.processing_date = processing_date or (
            datetime.now() - timedelta(days=1)
        )

        # Ensure output directory exists
        os.makedirs(output_path, exist_ok=True)

        logger.info(
            f"UserSegmentationJob initialized. "
            f"Input: {input_path}, Output: {output_path}, "
            f"Date: {self.processing_date.strftime('%Y-%m-%d')}"
        )

    def _read_parquet(self) -> DataFrame:
        """Read Parquet data from the input path.

        Returns:
            DataFrame: Raw clickstream events.

        Note:
            In production, you would filter by date partition.
            For demo purposes, we read all available data.
        """
        try:
            df = self.spark.read.parquet(self.input_path)
            logger.info(f"Read {df.count()} records from Parquet")
            return df
        except Exception as e:
            logger.error(f"Failed to read Parquet: {e}")
            # Return empty DataFrame with expected schema
            return self.spark.createDataFrame([], schema="""
                user_id STRING,
                product_id STRING,
                event_type STRING,
                timestamp STRING,
                category STRING,
                session_id STRING,
                event_timestamp TIMESTAMP
            """)

    def segment_users(self, df: DataFrame) -> DataFrame:
        """Segment users into Window Shoppers vs Buyers.

        Window Shoppers: Users who viewed products but never purchased.
        Buyers: Users who made at least one purchase.

        Args:
            df: Raw clickstream DataFrame.

        Returns:
            DataFrame: User segments with stats.
        """
        user_stats = df.groupBy("user_id").agg(
            count(when(col("event_type") == "view", 1)).alias("total_views"),
            count(when(col("event_type") == "add_to_cart", 1)).alias("total_carts"),
            count(when(col("event_type") == "purchase", 1)).alias("total_purchases"),
            countDistinct("product_id").alias("unique_products"),
            countDistinct("session_id").alias("sessions")
        )

        # Classify users
        segmented = user_stats.withColumn(
            "segment",
            when(col("total_purchases") > 0, "Buyer").otherwise("Window Shopper")
        ).withColumn(
            "engagement_score",
            (col("total_views") * 1 +
             col("total_carts") * 3 +
             col("total_purchases") * 10)
        )

        logger.info("User segmentation complete")
        return segmented

    def get_segment_summary(self, segmented_df: DataFrame) -> DataFrame:
        """Generate summary statistics for each user segment.

        Args:
            segmented_df: DataFrame with user segments.

        Returns:
            DataFrame: Summary by segment.
        """
        return segmented_df.groupBy("segment").agg(
            count("*").alias("user_count"),
            spark_round(
                spark_sum("total_views") / count("*"), 2
            ).alias("avg_views_per_user"),
            spark_round(
                spark_sum("total_purchases") / count("*"), 2
            ).alias("avg_purchases_per_user"),
            spark_round(
                spark_sum("engagement_score") / count("*"), 2
            ).alias("avg_engagement_score")
        )

    def get_top_viewed_products(
        self,
        df: DataFrame,
        limit: int = 5
    ) -> DataFrame:
        """Get the top N most viewed products.

        Args:
            df: Raw clickstream DataFrame.
            limit: Number of products to return.

        Returns:
            DataFrame: Top viewed products with stats.
        """
        views = df.filter(col("event_type") == "view")

        top_products = views.groupBy("product_id", "category").agg(
            count("*").alias("view_count"),
            countDistinct("user_id").alias("unique_viewers")
        ).orderBy(
            desc("view_count")
        ).limit(limit)

        return top_products

    def calculate_conversion_rates(self, df: DataFrame) -> DataFrame:
        """Calculate conversion rates per product category.

        Conversion Rate = (Purchases / Views) * 100

        Args:
            df: Raw clickstream DataFrame.

        Returns:
            DataFrame: Conversion rates by category.
        """
        category_stats = df.groupBy("category").agg(
            count(when(col("event_type") == "view", 1)).alias("views"),
            count(when(col("event_type") == "add_to_cart", 1)).alias("carts"),
            count(when(col("event_type") == "purchase", 1)).alias("purchases"),
            countDistinct("user_id").alias("unique_users")
        )

        # Calculate rates
        conversion = category_stats.withColumn(
            "view_to_cart_rate",
            spark_round(
                when(col("views") > 0,
                     col("carts") / col("views") * 100
                ).otherwise(0.0), 2
            )
        ).withColumn(
            "cart_to_purchase_rate",
            spark_round(
                when(col("carts") > 0,
                     col("purchases") / col("carts") * 100
                ).otherwise(0.0), 2
            )
        ).withColumn(
            "overall_conversion_rate",
            spark_round(
                when(col("views") > 0,
                     col("purchases") / col("views") * 100
                ).otherwise(0.0), 2
            )
        ).orderBy(desc("overall_conversion_rate"))

        return conversion

    def generate_email_summary(
        self,
        segment_summary: DataFrame,
        top_products: DataFrame,
        conversion_rates: DataFrame
    ) -> str:
        """Generate a formatted email summary report.

        Args:
            segment_summary: User segment statistics.
            top_products: Top viewed products.
            conversion_rates: Conversion rates by category.

        Returns:
            str: Formatted text summary.
        """
        date_str = self.processing_date.strftime("%Y-%m-%d")

        lines = [
            "=" * 60,
            f"E-COMMERCE DAILY ANALYTICS REPORT",
            f"Date: {date_str}",
            "=" * 60,
            "",
            "USER SEGMENTATION SUMMARY",
            "-" * 40,
        ]

        # Add segment data
        for row in segment_summary.collect():
            lines.extend([
                f"  {row['segment']}:",
                f"    User Count: {row['user_count']}",
                f"    Avg Views/User: {row['avg_views_per_user']}",
                f"    Avg Purchases/User: {row['avg_purchases_per_user']}",
                f"    Avg Engagement Score: {row['avg_engagement_score']}",
                ""
            ])

        lines.extend([
            "",
            "TOP 5 MOST VIEWED PRODUCTS",
            "-" * 40,
        ])

        # Add top products
        for i, row in enumerate(top_products.collect(), 1):
            lines.append(
                f"  {i}. {row['product_id']} ({row['category']})"
            )
            lines.append(
                f"     Views: {row['view_count']}, "
                f"Unique Viewers: {row['unique_viewers']}"
            )

        lines.extend([
            "",
            "CONVERSION RATES BY CATEGORY",
            "-" * 40,
        ])

        # Add conversion rates
        for row in conversion_rates.collect():
            lines.extend([
                f"  {row['category']}:",
                f"    Views: {row['views']}, Purchases: {row['purchases']}",
                f"    Overall Conversion: {row['overall_conversion_rate']}%",
                f"    View→Cart: {row['view_to_cart_rate']}%, "
                f"Cart→Purchase: {row['cart_to_purchase_rate']}%",
                ""
            ])

        lines.extend([
            "",
            "=" * 60,
            "Report Generated: " + datetime.now().isoformat(),
            "=" * 60,
        ])

        return "\n".join(lines)

    def save_reports(
        self,
        segment_summary: DataFrame,
        top_products: DataFrame,
        conversion_rates: DataFrame,
        email_summary: str
    ) -> Dict[str, str]:
        """Save all reports to the output directory.

        Args:
            segment_summary: User segment statistics.
            top_products: Top viewed products.
            conversion_rates: Conversion rates by category.
            email_summary: Formatted text summary.

        Returns:
            dict: Paths to all generated files.
        """
        date_str = self.processing_date.strftime("%Y%m%d")
        paths = {}

        # Save DataFrames as CSV
        segment_path = f"{self.output_path}/user_segments_{date_str}.csv"
        segment_summary.toPandas().to_csv(segment_path, index=False)
        paths["segment_summary"] = segment_path

        top_path = f"{self.output_path}/top_products_{date_str}.csv"
        top_products.toPandas().to_csv(top_path, index=False)
        paths["top_products"] = top_path

        conversion_path = f"{self.output_path}/conversion_rates_{date_str}.csv"
        conversion_rates.toPandas().to_csv(conversion_path, index=False)
        paths["conversion_rates"] = conversion_path

        # Save email summary
        summary_path = f"{self.output_path}/daily_summary_{date_str}.txt"
        with open(summary_path, "w") as f:
            f.write(email_summary)
        paths["email_summary"] = summary_path

        logger.info(f"Reports saved: {paths}")
        return paths

    def run(self) -> dict:
        """Execute the complete user segmentation pipeline.

        Returns:
            dict: Results including paths and statistics.
        """
        logger.info("Starting User Segmentation Job")

        # Read data
        raw_df = self._read_parquet()

        if raw_df.count() == 0:
            logger.warning("No data found for processing")
            return {"status": "no_data", "paths": {}}

        # Cache for multiple operations
        raw_df.cache()

        # Run analytics
        segmented_users = self.segment_users(raw_df)
        segment_summary = self.get_segment_summary(segmented_users)
        top_products = self.get_top_viewed_products(raw_df, limit=5)
        conversion_rates = self.calculate_conversion_rates(raw_df)

        # Generate summary
        email_summary = self.generate_email_summary(
            segment_summary, top_products, conversion_rates
        )

        # Save reports
        paths = self.save_reports(
            segment_summary, top_products, conversion_rates, email_summary
        )

        # Uncache
        raw_df.unpersist()

        logger.info("User Segmentation Job complete")

        return {
            "status": "success",
            "date": self.processing_date.strftime("%Y-%m-%d"),
            "paths": paths,
            "stats": {
                "total_events": raw_df.count(),
                "segments": segment_summary.count(),
                "categories": conversion_rates.count()
            }
        }


def main() -> None:
    """Main entry point for the batch job.

    This function is called by Airflow SparkSubmitOperator.
    """
    # Load settings
    sys.path.insert(0, "/opt/spark-apps")
    try:
        from config.settings import get_settings
        settings = get_settings()
        input_path = settings.spark.parquet_output_path
        output_path = settings.airflow.reports_output_path
    except ImportError:
        logger.warning("Settings not found, using defaults")
        input_path = "/opt/spark-data/parquet"
        output_path = "/opt/spark-reports"

    # Create Spark session
    spark = SparkSession.builder \
        .appName("UserSegmentationBatchJob") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # Run job
    job = UserSegmentationJob(
        spark=spark,
        input_path=input_path,
        output_path=output_path
    )

    try:
        results = job.run()
        logger.info(f"Job results: {results}")

        if results["status"] == "success":
            print("\n" + "=" * 60)
            print("USER SEGMENTATION JOB COMPLETED SUCCESSFULLY")
            print("=" * 60)
            print(f"Date Processed: {results['date']}")
            print(f"Total Events: {results['stats']['total_events']}")
            print(f"\nOutput Files:")
            for name, path in results["paths"].items():
                print(f"  - {name}: {path}")
            print("=" * 60)
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
