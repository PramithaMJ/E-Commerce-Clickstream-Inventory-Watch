# E-COMMERCE CLICKSTREAM PIPELINE - ENVIRONMENT CONFIGURATION
# 
# INSTRUCTIONS:
# 1. Copy this file to .env: cp .env.example .env
# 2. Modify the values below according to your environment
# 3. DO NOT commit .env to version control (it's in .gitignore)
#


# KAFKA CONFIGURATION
# Message broker settings for event streaming

# Kafka broker address (use 'broker:29092' for Docker, 'localhost:9092' for local)
KAFKA_BOOTSTRAP_SERVERS=broker:29092

# Topic for raw clickstream events (views, add_to_cart, purchases)
KAFKA_CLICKSTREAM_TOPIC=clickstream_events

# Topic for flash sale alert notifications
KAFKA_ALERTS_TOPIC=flash_sale_alerts

# Consumer group ID for Spark streaming
KAFKA_CONSUMER_GROUP=ecommerce_processor

# Number of partitions for clickstream topic
KAFKA_PARTITIONS=3

# Replication factor (set to 1 for single broker, 3 for production)
KAFKA_REPLICATION_FACTOR=1


# SPARK CONFIGURATION
# Distributed processing engine settings

# Spark master URL
SPARK_MASTER_URL=spark://spark-master:7077

# Application name (appears in Spark UI)
SPARK_APP_NAME=EcommerceClickstreamProcessor

# Sliding window duration in minutes (must be > slide duration)
SPARK_WINDOW_DURATION=10

# Slide interval in minutes (how often windows are evaluated)
SPARK_SLIDE_DURATION=5

# Watermark delay for late event tolerance
SPARK_WATERMARK_DELAY=2 minutes

# Checkpoint directory for fault tolerance
SPARK_CHECKPOINT_LOCATION=/opt/spark-data/checkpoints

# Parquet output path for batch layer
SPARK_PARQUET_OUTPUT_PATH=/opt/spark-data/parquet

# Worker memory allocation
SPARK_WORKER_MEMORY=2G

# Worker CPU cores
SPARK_WORKER_CORES=2


# ALERT CONFIGURATION
# Flash sale detection thresholds

# Minimum views to consider "high interest" (trigger threshold)
ALERT_MIN_VIEWS_THRESHOLD=100

# Maximum purchases to consider "low conversion" (trigger threshold)
ALERT_MAX_PURCHASES_THRESHOLD=5

# Alert notification email (for future email integration)
ALERT_NOTIFICATION_EMAIL=alerts@example.com


# PRODUCER CONFIGURATION
# Data generator settings for synthetic events

# Events generated per second (adjust for load testing)
PRODUCER_EVENTS_PER_SECOND=10.0

# Products to skew with high views (comma-separated)
PRODUCER_HIGH_INTEREST_PRODUCT_IDS=PROD_001,PROD_002,PROD_003

# Probability of generating high interest product events (0.0 - 1.0)
PRODUCER_HIGH_INTEREST_PROBABILITY=0.3

# Number of simulated users
PRODUCER_NUM_USERS=1000

# Number of simulated products
PRODUCER_NUM_PRODUCTS=100

# Producer run duration in seconds
PRODUCER_DURATION_SECONDS=300


# AIRFLOW CONFIGURATION
# Workflow orchestration settings

# DAG identifier
AIRFLOW_DAG_ID=ecommerce_daily_segmentation

# Cron schedule (default: 2 AM daily)
AIRFLOW_SCHEDULE_INTERVAL=0 2 * * *

# Reports output directory
AIRFLOW_REPORTS_OUTPUT_PATH=/opt/airflow/reports

# Airflow executor type
AIRFLOW_EXECUTOR=LocalExecutor


# POSTGRESQL DATABASE (Airflow metadata)
# CHANGE THESE IN PRODUCTION!
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Full connection string (auto-generated from above)
AIRFLOW_DATABASE_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}


# AIRFLOW WEB UI CREDENTIALS
# CHANGE THESE IN PRODUCTION!
AIRFLOW_ADMIN_USERNAME=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_ADMIN_EMAIL=admin@example.com


# APPLICATION SETTINGS

# Application name
APP_NAME=E-Commerce Clickstream Pipeline

# Environment (development, staging, production)
ENVIRONMENT=development

# Enable debug mode (true/false)
DEBUG=true

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO


# SERVER CONFIGURATION

# Public IP address of the server (for documentation and access)
SERVER_PUBLIC_IP=13.235.248.201

# Port mappings
AIRFLOW_WEB_PORT=8080
SPARK_MASTER_PORT=8081
SPARK_WORKER_PORT=8082
KAFKA_EXTERNAL_PORT=9092
