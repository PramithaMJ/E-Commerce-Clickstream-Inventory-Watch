# E-Commerce Clickstream & Inventory Watch Pipeline

A complete **Lambda Architecture** implementation for real-time e-commerce analytics with batch user segmentation.

> **Live Server**: http://13.235.248.201

## ðŸŒ Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow UI** | http://13.235.248.201:8080 | admin / admin |
| **Spark Master UI** | http://13.235.248.201:8081 | - |
| **Spark Worker UI** | http://13.235.248.201:8082 | - |

---

## ðŸ“ Architecture Overview

This pipeline implements the **Lambda Architecture** pattern, combining real-time stream processing with batch analytics.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        E-COMMERCE CLICKSTREAM PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Data Producer   â”‚  Generates synthetic clickstream events:
  â”‚  (Python Script) â”‚  â€¢ user_id, product_id, event_type, timestamp
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Apache Kafka   â”‚  Message broker for event ingestion
  â”‚  (Broker:29092)  â”‚  â€¢ Topic: clickstream_events
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                                                 â”‚
           â–¼                                                 â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   SPEED LAYER          â”‚                    â”‚   BATCH LAYER          â”‚
  â”‚   (Real-Time)          â”‚                    â”‚   (Historical)         â”‚
  â”‚                        â”‚                    â”‚                        â”‚
  â”‚  Spark Structured      â”‚                    â”‚  Parquet Files         â”‚
  â”‚  Streaming             â”‚                    â”‚  (Partitioned by       â”‚
  â”‚  â€¢ 10-min windows      â”‚                    â”‚   category)            â”‚
  â”‚  â€¢ 5-min slide         â”‚                    â”‚                        â”‚
  â”‚  â€¢ 2-min watermark     â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                        â”‚                                â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚ Flash Sale Alert â”‚  â”‚                    â”‚   Apache Airflow       â”‚
  â”‚  â”‚ Detection        â”‚  â”‚                    â”‚   Daily DAG            â”‚
  â”‚  â”‚ views>100 &      â”‚  â”‚                    â”‚   (2 AM UTC)           â”‚
  â”‚  â”‚ purchases<5      â”‚  â”‚                    â”‚                        â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚  â€¢ User Segmentation   â”‚
  â”‚                        â”‚                    â”‚  â€¢ Top 5 Products      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â€¢ Conversion Rates    â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ›  Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Message Broker** | Apache Kafka 7.5.0 | Real-time event ingestion |
| **Stream Processing** | Apache Spark 3.5.0 | Structured Streaming with Event Time |
| **Orchestration** | Apache Airflow 2.7.1 | Batch job scheduling |
| **Storage** | Parquet + PostgreSQL | Columnar storage & metadata |
| **Coordination** | Apache Zookeeper | Kafka coordination |
| **Containerization** | Docker Compose | Service orchestration |

### Why This Stack?

1. **Kafka**: Handles high-throughput, fault-tolerant event streaming with partitioning support
2. **Spark Structured Streaming**: Provides exactly-once processing with event time semantics
3. **Airflow**: Enterprise-grade DAG orchestration with monitoring and retry logic
4. **Parquet**: Columnar format optimized for analytical queries with compression

---

## â± Event Time vs Processing Time

This pipeline uses **Event Time** (when events occurred) for accurate analytics:

```python
# Event Time is embedded in each message
{
    "user_id": "USER_0123",
    "product_id": "PROD_001",
    "event_type": "view",
    "timestamp": "2026-01-26T07:10:15.123456+00:00",  # â† EVENT TIME
    "category": "smartphones"
}
```

### Why Event Time?

| Aspect | Event Time | Processing Time |
|--------|------------|-----------------|
| **Accuracy** | Reflects actual user behavior | May include processing delays |
| **Late Events** | Handled via watermarking | Lost or misattributed |
| **Reproducibility** | Same input â†’ Same output | Non-deterministic |

### Watermarking

```python
# Allow events arriving up to 2 minutes late
df.withWatermark("event_timestamp", "2 minutes")

# 10-minute sliding windows
window(col("event_timestamp"), "10 minutes", "5 minutes")
```

---

## ðŸš€ Quick Start Guide

### Prerequisites

- Docker & Docker Compose
- 8GB+ RAM recommended
- Ports 8080, 8081, 8082, 9092 open

### Step 1: Start All Services

```bash
cd ecommerce_pipeline

# Start the stack
sudo docker compose up -d

# Wait for services to be healthy (2-3 minutes)
sudo docker compose ps
```

### Step 2: Run the Kafka Producer

Generate synthetic clickstream events:

```bash
sudo docker exec spark-worker python3 /opt/spark-apps/src/producers/kafka_producer.py
```

This generates events at **10 events/second** for **5 minutes** with:
- 1000 simulated users
- 100 products across 6 categories
- Skewed data toward specific "high interest" products (PROD_001, PROD_002, PROD_003)

### Step 3: Start Spark Streaming Processor

```bash
sudo docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/src/streaming/spark_processor.py
```

This starts:
- **Windowed aggregations** (views/purchases per product)
- **Flash Sale Alert detection** (views > 100, purchases < 5)
- **Parquet archival** (for batch processing)

### Step 4: Trigger Airflow Batch Job

1. Open Airflow: http://13.235.248.201:8080
2. Login with `admin` / `admin`
3. Enable and trigger `ecommerce_daily_segmentation` DAG

---

## ðŸ“Š Pipeline Outputs

### 1. Real-Time Console Output (Spark Streaming)

```
+------------------------------------------+----------+-----------+----------+--------------+
|window                                    |product_id|category   |view_count|purchase_count|
+------------------------------------------+----------+-----------+----------+--------------+
|{2026-01-26 07:05:00, 2026-01-26 07:15:00}|PROD_001  |smartphones|156       |3             |
|{2026-01-26 07:05:00, 2026-01-26 07:15:00}|PROD_002  |gaming     |142       |2             |
+------------------------------------------+----------+-----------+----------+--------------+
```

### 2. Flash Sale Alerts

```
ðŸš¨ FLASH SALE ALERT ðŸš¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Product ID: PROD_001
Category: smartphones
Views: 156
Purchases: 3
Conversion Rate: 1.92%
Suggestion: Consider Flash Sale!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 3. Daily Batch Reports (Airflow)

Generated in `/opt/airflow/reports/`:
- `user_segments_YYYYMMDD.csv` - Window Shoppers vs Buyers
- `top_products_YYYYMMDD.csv` - Top 5 most viewed products
- `conversion_rates_YYYYMMDD.csv` - Rates by category
- `daily_summary_YYYYMMDD.txt` - Email-ready summary

---

## ðŸ“ Project Structure

```
ecommerce_pipeline/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py              # Singleton Pydantic configuration
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ spark/Dockerfile         # Custom Spark image with Kafka
â”‚   â””â”€â”€ airflow/Dockerfile       # Custom Airflow image
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ecommerce_daily_dag.py   # Airflow DAG definition
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producers/
â”‚   â”‚   â”œâ”€â”€ data_generator.py    # Factory Pattern event generator
â”‚   â”‚   â””â”€â”€ kafka_producer.py    # Kafka message producer
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ spark_processor.py   # Spark Structured Streaming
â”‚   â”‚   â””â”€â”€ alert_handler.py     # Strategy Pattern alert handlers
â”‚   â””â”€â”€ batch/
â”‚       â”œâ”€â”€ user_segmentation.py # PySpark batch analytics
â”‚       â””â”€â”€ report_generator.py  # Factory Pattern reports
â”œâ”€â”€ data/                        # Parquet output (volume mounted)
â”œâ”€â”€ reports/                     # Generated reports (volume mounted)
â”œâ”€â”€ docker-compose.yaml          # Service definitions
â”œâ”€â”€ .env                         # Environment configuration
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ðŸŽ¯ Design Patterns Used

| Pattern | Implementation | Purpose |
|---------|----------------|---------|
| **Factory** | `DataGeneratorFactory`, `ReportFactory` | Create objects without exposing creation logic |
| **Singleton** | `AppSettings` | Single configuration instance |
| **Strategy** | `AlertHandler` implementations | Interchangeable alert handling algorithms |
| **Builder** | Spark DataFrame transformations | Construct complex queries step-by-step |

---

## âš™ï¸ Configuration

All settings are managed via environment variables or `.env` file:

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVERS=broker:29092
KAFKA_CLICKSTREAM_TOPIC=clickstream_events

# Spark
SPARK_WINDOW_DURATION=10
SPARK_SLIDE_DURATION=5
SPARK_WATERMARK_DELAY=2 minutes

# Alert Thresholds
ALERT_MIN_VIEWS_THRESHOLD=100
ALERT_MAX_PURCHASES_THRESHOLD=5

# Producer
PRODUCER_EVENTS_PER_SECOND=10.0
PRODUCER_HIGH_INTEREST_PRODUCT_IDS=PROD_001,PROD_002,PROD_003
```

---

## ðŸ”’ Ethics & Data Governance

### Privacy Implications

E-commerce clickstream data contains sensitive user behavior information:

1. **User Profiling Risks**: Tracking browsing patterns can reveal personal preferences, financial status, and shopping habits
2. **Data Retention**: How long should clickstream data be kept?
3. **Consent**: Users should be informed about data collection

### Recommended Governance Practices

| Practice | Implementation |
|----------|----------------|
| **Anonymization** | Hash user_id before storage |
| **Data Minimization** | Collect only necessary fields |
| **Retention Policy** | Auto-delete data after 30 days |
| **Access Control** | Role-based access to reports |
| **Audit Logging** | Track who accessed what data |

---

## ðŸ›‘ Stopping the Pipeline

```bash
# Stop all services
sudo docker compose down

# Stop and remove volumes
sudo docker compose down -v
```

---

## ðŸ“ Troubleshooting

| Issue | Solution |
|-------|----------|
| Producer can't connect to Kafka | Ensure broker uses `broker:29092` (Docker network) |
| Spark streaming fails | Check `/opt/spark-data` directory permissions |
| Airflow DAG shows "no status" | Ensure Parquet data exists in `/opt/spark-data/parquet` |
| Type errors in Python | Python 3.9 compatibility - use `List[str]` not `list[str]` |

---

## ðŸ“œ License

MIT License

---

## ðŸ‘¥ Contributors

This project was built as part of an Applied Big Data Engineering course to demonstrate Lambda Architecture implementation.
